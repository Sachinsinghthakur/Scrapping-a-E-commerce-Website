import scrapy
import json
from math import ceil
import requests

class leclerc_products(scrapy.Spider):

    name = "leclerc"

    # XHR url as given website is dynamic so we will start by sending a request to get the sub categories name in our two categories
    # that is "sports" and "jewel" 

    start_urls= [ "https://www.e.leclerc/api/rest/live-api/categories-tree-by-code/NAVIGATION_sport-loisirs?pageType=null&maxDepth=3",
                    "https://www.e.leclerc/api/rest/live-api/categories-tree-by-code/NAVIGATION_vetements?pageType=null&maxDepth=3"]
    
    #important headers field to get the response from the server

    headers = { "accept":'application/json,text/plain,*/*',
                "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
                }


    def parse(self,response):

        #extract_categories function scrap the list of all subcategories along with their product's page
        categories = self.extract_categories(response)


#for each category we will go through its product's url list to scrap each product

        for category in categories:
        
            for urll in category["url_list"]:  #category["url_list"] contains the all product's page urls in category

                data = requests.get(urll,headers=self.headers)
                raw_data= data.json()
                
                for item in raw_data["items"]:

                    product={}
                    
                    #we are also passing category name with the product to keep track of category it belongs
                    #    while inserting the product into the collection 

                    product["collection"] = category["category"]

                    #product_name
                    product["name"]=item["label"]
                    
                    #product_brand
                    for attr in item["attributeGroups"][0]["attributes"]:
                        if attr["code"]=="marque":
                            product["brand"]=attr["value"]["label"]
                    
                    #product_price
                    product["original_price"]=item["variants"][0]["offers"][0]["basePrice"]["totalPrice"]["price"]

                    
                    #sale_price
                    try:
                        product["sale_price"]=item["variants"][0]["offers"][0]["basePrice"]["discountPrice"]["price"]["price"]
                    except KeyError:
                        product["sale_price"]=product["original_price"]
                    
                    
                    #list product's image url
                    product["img_urls"]=[]
                    
                    for i in item["variants"][0]["attributes"]:
                        if i["type"]=="image":
                            product["img_urls"].append(i["value"]["url"])
                    
                    product["product_page_url"]=str(urll)
                    
                    product["product_category"]=item["families"][1]["label"]
                    

                    #stock_status = True/False
                    if item["variants"][0]["offers"][0]["additionalFields"][1]["value"]=='in-stock':
                        product["stock"]=True
                    else:
                        product["stock"]=False

                    #ean
                    product["ean"]=item["sku"]

                    #sku
                    product["sku"]=item["id"]


                    yield product
    

#extract_categories - take response generated by start_urls from parse function and returns the list of categories
#                       along with their product's page url

    def extract_categories(self,response):

        raw_data = response.json()
        categories=[]

        for data in raw_data["children"]:

            # each category is represented as a dictionary with keys 
            # - "category" -> category name
            # - "number of page" to scrap 
            # - "url_list" of all product pages to scrap

            dct = {}

            # dct["category"]=data["description"]
            # dct["category"]=data["label"]

#the reason for not taking description and label field as category name was mongodb having problem droping
#               the database names containing ' above characters


            dct["category"]=data["code"][11:].replace("-"," ")
            code=data["code"]

            #calculating the number of product's pages without scrapping it from website

            num_of_products=data["nbProducts"]
            num_of_page=ceil(num_of_products/30)
            dct["number_of_page"] = num_of_page
            
            base_url="https://www.e.leclerc/api/rest/live-api/product-search?language=fr-FR&size=30&sorts=%5B%5D&page="
            
            suffix_url=code[11:]

            #generating the list of product's pages  XHR request by following the requests pattern

            list_of_urls=[]

            for i in range(1,dct["number_of_page"]+1):
                url=base_url+str(i)+"&categories=%7B%22code%22:%5B%22"+code+"%22%5D%7D"
                list_of_urls.append(url)
            
            dct["url_list"]=list_of_urls

            categories.append(dct)

        return categories