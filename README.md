# cliff.ai Take Home Assignment

# STEPS I FOLLOWED TO SOLVE THE ASSIGNMENT :

- As ususal I started scraping the given website using CS tags and XPATH as I never came across a dynamic website to scrap and didn't know much about how to scrap it.
- After I learned it's a dynamic website I read and watched many youtube videos that can help me to get started with scraping dynamic website.
- I decided to go with XHR request that are sent to server to load website dynamically.
- Everything in the project was new to me so I decided to write and test my code as granular as possible using Jupyter and done all testing in test.ipynp
- 
- Suggested READMEs should be beautiful or stand out in some way.
- Make an individual pull request for each suggestion.
- New categories, or improvements to the existing categorization are welcome.
- Keep descriptions short and simple, but descriptive.
- Start the description with a capital and end with a full stop/period.
- Check your spelling and grammar.
- Make sure your text editor is set to remove trailing whitespace.
- Use the `#readme` anchor for GitHub READMEs to link them directly

Thank you for your suggestions!

# File and Folder Structure

- I followed the default srapy's project structure for the assignment.
- All scrapy-related code is placed directly in project folder leclercProducts and subdirectory with the same name (leclercProducts).
- The Screenshots folder along with project folder LeclercProducts contain the screenshot of terminal with request and response and I also added the screenshot
  of my cloud mongo db cluster images containing the database and all its collection.
- The spider created for crawling the web pages is in the spider folder with name leclerc_spider.py.
- pipeline.py and settings.py file is edited corrospond to the leclerc_spider.py
